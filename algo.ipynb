{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb60172",
   "metadata": {},
   "source": [
    "# Linear Inverse Problems via Flow Models\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### Theorem 1: Conditional Vector Fields under Gaussian Probability Paths\n",
    "\n",
    "For a Gaussian probability path $q$, if we observe measurements $\\mathbf{y} \\sim q(\\mathbf{y} \\mid \\mathbf{x}_1)$ and have an unconditional vector field $v(\\mathbf{x}_t)$ enabling sampling $\\mathbf{x}_t \\sim q(\\mathbf{x}_t)$, then the conditional vector field $v(\\mathbf{x}_t, \\mathbf{y})$ enabling sampling $\\mathbf{x}_t \\sim q(\\mathbf{x}_t \\mid \\mathbf{y})$ is:\n",
    "\n",
    "$$v(\\mathbf{x}_t, \\mathbf{y}) = v(\\mathbf{x}_t) + \\sigma_t^2 \\frac{d \\ln(\\alpha_t / \\sigma_t)}{dt} \\nabla_{\\mathbf{x}_t} \\ln q(\\mathbf{y} \\mid \\mathbf{x}_t)$$\n",
    "\n",
    "### Training-Free Algorithm Adaptation\n",
    "\n",
    "Using a pretrained unconditional denoiser/vector field $\\widehat{v}(\\mathbf{x}_t)$ or $\\widehat{x}_1(\\mathbf{x}_t)$, we approximate the conditional vector field as:\n",
    "\n",
    "$$\\widehat{v}(\\mathbf{x}_t, \\mathbf{y}) = \\widehat{v}(\\mathbf{x}_t) + \\sigma_t^2 \\frac{d \\ln(\\alpha_t / \\sigma_t)}{dt} \\gamma_t \\nabla_{\\mathbf{x}_t} \\ln q^{app}(\\mathbf{y} \\mid \\mathbf{x}_t)$$\n",
    "\n",
    "where $\\gamma_t$ is an adaptive weight (often set to 1 for unadaptive case) that accounts for approximation errors in the likelihood.\n",
    "\n",
    "### Likelihood Approximation for Linear Measurements\n",
    "\n",
    "For linear measurements with Gaussian approximation on $q(\\mathbf{x}_1 \\mid \\mathbf{x}_t) \\approx \\mathcal{N}(\\widehat{\\mathbf{x}}_1(\\mathbf{x}_t), r_t^2 \\mathbf{I})$:\n",
    "\n",
    "$$q^{app}(\\mathbf{y} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{A}\\widehat{\\mathbf{x}}_1(\\mathbf{x}_t), \\sigma_y^2 \\mathbf{I} + r_t^2 \\mathbf{A}\\mathbf{A}^\\top)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$r_t^2 = \\frac{\\sigma_t^2}{\\sigma_t^2 + \\alpha_t^2}$$\n",
    "\n",
    "The gradient of the log-likelihood is:\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}_t} \\ln q^{app}(\\mathbf{y} \\mid \\mathbf{x}_t) = (\\mathbf{y} - \\mathbf{A}\\widehat{\\mathbf{x}}_1(\\mathbf{x}_t))^\\top (\\sigma_y^2 \\mathbf{I} + r_t^2 \\mathbf{A}\\mathbf{A}^\\top)^{-1} \\mathbf{A} \\frac{\\partial \\widehat{\\mathbf{x}}_1}{\\partial \\mathbf{x}_t}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46447e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128558b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install pillow opencv-python matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e067f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Callable, Tuple, Optional, Dict\n",
    "from scipy.integrate import odeint\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GaussianProbabilityPath:\n",
    "    \"\"\"\n",
    "    Represents a Gaussian probability path with time-dependent noise schedule.\n",
    "    \n",
    "    Implements path: q(x_t | x_1) = N(α_t x_1, σ_t^2 I)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_fn: Callable, sigma_fn: Callable):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha_fn: Function mapping t -> α_t (scaling factor)\n",
    "            sigma_fn: Function mapping t -> σ_t (noise level)\n",
    "        \"\"\"\n",
    "        self.alpha_fn = alpha_fn\n",
    "        self.sigma_fn = sigma_fn\n",
    "    \n",
    "    def alpha(self, t: float) -> float:\n",
    "        \"\"\"Get α_t at time t\"\"\"\n",
    "        return self.alpha_fn(t)\n",
    "    \n",
    "    def sigma(self, t: float) -> float:\n",
    "        \"\"\"Get σ_t at time t\"\"\"\n",
    "        return self.sigma_fn(t)\n",
    "    \n",
    "    def r_t_squared(self, t: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute r_t^2 from Eq. (13) and paper derivation:\n",
    "        r_t^2 = σ_t^2 / (σ_t^2 + α_t^2)\n",
    "        \"\"\"\n",
    "        alpha_t = self.alpha(t)\n",
    "        sigma_t = self.sigma(t)\n",
    "        return (sigma_t ** 2) / (sigma_t ** 2 + alpha_t ** 2)\n",
    "    \n",
    "    def d_log_alpha_sigma_dt(self, t: float, eps: float = 1e-5) -> float:\n",
    "        \"\"\"\n",
    "        Compute d ln(α_t / σ_t) / dt using finite differences\n",
    "        \n",
    "        This is needed for Theorem 1's correction term\n",
    "        \"\"\"\n",
    "        log_ratio_plus = np.log(self.alpha(t + eps) / self.sigma(t + eps))\n",
    "        log_ratio = np.log(self.alpha(t) / self.sigma(t))\n",
    "        return (log_ratio_plus - log_ratio) / eps\n",
    "\n",
    "\n",
    "class ConditionalOTFlowSolver(nn.Module):\n",
    "    \"\"\"\n",
    "    Solves linear inverse problems via flows using conditional OT probability path.\n",
    "    \n",
    "    This implements Algorithm 1 based on Theorem 1 in the paper, using a pretrained \n",
    "    denoiser converted to a conditional OT probability path.\n",
    "    \n",
    "    Dimension agnostic: Works with signals of any shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        denoiser: Callable,\n",
    "        measurement_matrix: torch.Tensor,\n",
    "        probability_path: GaussianProbabilityPath,\n",
    "        sigma_y: float = 0.0,\n",
    "        gamma_t: float = 1.0,\n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            denoiser: Pretrained denoiser model x̂_1(z_t) that takes noisy input and returns denoised output\n",
    "            measurement_matrix: Measurement matrix A (shape: [m, n] where n is the flattened signal dimension)\n",
    "            probability_path: GaussianProbabilityPath defining the noise schedule\n",
    "            sigma_y: Standard deviation of measurement noise\n",
    "            gamma_t: Adaptive weight for likelihood correction (typically 1.0)\n",
    "            device: Device to run computations on ('cpu' or 'cuda')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.denoiser = denoiser\n",
    "        self.register_buffer('A', measurement_matrix)\n",
    "        self.probability_path = probability_path\n",
    "        self.sigma_y = sigma_y\n",
    "        self.gamma_t = gamma_t\n",
    "        self.device = device\n",
    "        \n",
    "    def initialize_xt(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        t: float,\n",
    "        shape: Tuple[int, ...]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Initialize x_t in signal space using Eq. (14): z_t = t*y_lifted + (1-t)*ε, where ε ~ N(0, I)\n",
    "        \n",
    "        Since y is in measurement space (shape [m]) but we need signal space (shape [...]),\n",
    "        we use A^† y (pseudo-inverse) as a rough initialization in signal space.\n",
    "        \n",
    "        Args:\n",
    "            y: Noisy measurements (shape: [m])\n",
    "            t: Initial time step in [0, 1]\n",
    "            shape: Shape of the full signal (e.g., (n,) for 1D, (h, w) for 2D, (d, h, w) for 3D, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            z_t: Initialized noisy signal in signal space with specified shape\n",
    "        \"\"\"\n",
    "        y = y.to(self.device)\n",
    "        \n",
    "        # Sample random noise ε ~ N(0, I) in signal space\n",
    "        epsilon = torch.randn(shape, device=self.device, dtype=self.A.dtype)\n",
    "        \n",
    "        # Lift y to signal space using pseudo-inverse\n",
    "        # Flatten shape to get total signal dimension\n",
    "        n_signal = int(np.prod(shape))\n",
    "        A_pinv = torch.linalg.pinv(self.A)  # Shape: [n, m]\n",
    "        y_lifted = A_pinv @ y  # Shape: [n]\n",
    "        \n",
    "        # Reshape to desired shape\n",
    "        y_lifted = y_lifted.reshape(shape)\n",
    "        \n",
    "        # Initialize: blend between lifted measurement and noise\n",
    "        z_t = self.probability_path.alpha(t) * y_lifted + self.probability_path.sigma(t) * epsilon\n",
    "            \n",
    "        return z_t\n",
    "    \n",
    "    def compute_r_t_squared(self, t_prime: float) -> float:\n",
    "        r\"\"\"\n",
    "        Compute r_t'^2 from the probability path.\n",
    "        \n",
    "        Eq. 13:\n",
    "        r_t^2=\\frac{\\sigma_t^2}{\\sigma_t^2+\\alpha_t^2}\n",
    "        \n",
    "        Or should this be simplified to:\n",
    "        r_{t^{\\prime}}^2=\\frac{\\left(1-t^{\\prime}\\right)^2}{t^{\\prime 2}+\\left(1-t^{\\prime}\\right)^2}\n",
    "        ??\n",
    "        \n",
    "        Args:\n",
    "            t_prime: Current time step\n",
    "            \n",
    "        Returns:\n",
    "            r_t'^2 value\n",
    "        \"\"\"\n",
    "        return self.probability_path.r_t_squared(t_prime)\n",
    "    \n",
    "    def convert_to_vector_field(\n",
    "        self,\n",
    "        z_t: torch.Tensor,\n",
    "        t_prime: float,\n",
    "        x_pred: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Convert denoiser prediction to vector field\n",
    "        \n",
    "        Eq 8: \n",
    "        \\widehat{\\boldsymbol{v}}=\\left(\\alpha_t \\frac{d \\ln \\left(\\alpha_t / \\sigma_t\\right)}{d t}\\right) \\widehat{\\boldsymbol{x}_1}+\\frac{d \\ln \\sigma_t}{d t} \\boldsymbol{x}_t\n",
    "\n",
    "        Simplified:\n",
    "        \\widehat{\\boldsymbol{v}}=\\frac{\\widehat{\\boldsymbol{x}}_1\\left(\\boldsymbol{z}_{t^{\\prime}}\\right)-\\boldsymbol{z}_{t^{\\prime}}}{1-t^{\\prime}}\n",
    "        \n",
    "        Args:\n",
    "            z_t: Current state (shape can be arbitrary)\n",
    "            t_prime: Current time step\n",
    "            x_pred: Denoiser prediction x̂_1(z_t') (same shape as z_t)\n",
    "            \n",
    "        Returns:\n",
    "            Vector field v̂ (same shape as z_t)\n",
    "        \"\"\"\n",
    "        if t_prime >= 1.0:\n",
    "            return torch.zeros_like(z_t)\n",
    "        \n",
    "        v_hat = (x_pred - z_t) / (1 - t_prime)\n",
    "        return v_hat\n",
    "    \n",
    "    def _flatten_and_unflatten(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, ...]]:\n",
    "        \"\"\"\n",
    "        Flatten a tensor to 1D and return the original shape.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of arbitrary shape\n",
    "            \n",
    "        Returns:\n",
    "            Flattened tensor, original shape\n",
    "        \"\"\"\n",
    "        original_shape = x.shape\n",
    "        flattened = x.reshape(-1)\n",
    "        return flattened, original_shape\n",
    "    \n",
    "    def compute_likelihood_gradient(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        x_pred: torch.Tensor,\n",
    "        z_t: torch.Tensor,\n",
    "        t_prime: float,\n",
    "        r_t_squared: float,\n",
    "        jacobian: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the likelihood gradient ∇_{x_t} ln q^app(y | x_t) from Theorem 1.\n",
    "        \n",
    "        This is the gradient of log-likelihood for the Gaussian measurement model:\n",
    "        q^app(y | x_t) = N(A x̂_1(x_t), σ_y^2 I + r_t^2 A A^T)\n",
    "        \n",
    "        Works with signals of arbitrary shape (flattens internally).\n",
    "        \n",
    "        Args:\n",
    "            y: Measurements (shape: [m])\n",
    "            x_pred: Denoiser prediction x̂_1(z_t') (arbitrary shape)\n",
    "            z_t: Current state (arbitrary shape)\n",
    "            t_prime: Current time step\n",
    "            r_t_squared: r_t'^2 value\n",
    "            jacobian: Pre-computed Jacobian (∂x̂_1/∂z_t'). If None, uses finite differences.\n",
    "            \n",
    "        Returns:\n",
    "            Likelihood gradient (same shape as z_t)\n",
    "        \"\"\"\n",
    "        y = y.to(self.device)\n",
    "        x_pred_flat, original_shape = self._flatten_and_unflatten(x_pred)\n",
    "        z_t_flat, _ = self._flatten_and_unflatten(z_t)\n",
    "        \n",
    "        # Residual: y - A x̂_1\n",
    "        residual = y - self.A @ x_pred_flat  # Shape: [m]\n",
    "        \n",
    "        # Covariance matrix: σ_y^2 I + r_t^2 A A^T\n",
    "        m = self.A.shape[0]\n",
    "        covariance = (\n",
    "            r_t_squared * (self.A @ self.A.T) +\n",
    "            (self.sigma_y ** 2) * torch.eye(m, device=self.device, dtype=self.A.dtype)\n",
    "        )\n",
    "        \n",
    "        # Compute Jacobian if not provided\n",
    "        if jacobian is None:\n",
    "            jacobian = self._compute_jacobian_fd(z_t, x_pred)\n",
    "        \n",
    "        # Likelihood gradient: ∇_{z_t} ln q = jacobian^T @ A^T @ covariance^{-1} @ residual\n",
    "        # where covariance = σ_y^2 I + r_t^2 A A^T\n",
    "        inv_cov_residual = torch.linalg.solve(covariance, residual)  # Shape: [m]\n",
    "        grad_measurement_space = self.A.T @ inv_cov_residual  # Shape: [n]\n",
    "        grad_likelihood = jacobian.T @ grad_measurement_space  # Shape: [n]\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        grad_likelihood = grad_likelihood.reshape(original_shape)\n",
    "        \n",
    "        return grad_likelihood\n",
    "    \n",
    "    def _compute_jacobian_fd(\n",
    "        self,\n",
    "        z_t: torch.Tensor,\n",
    "        x_pred: torch.Tensor,\n",
    "        eps: float = 1e-4\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Jacobian ∂x̂_1/∂z_t using finite differences.\n",
    "        \n",
    "        Dimension agnostic: Works with signals of arbitrary shape.\n",
    "        \n",
    "        Args:\n",
    "            z_t: Input state (arbitrary shape)\n",
    "            x_pred: Denoiser output at z_t (same shape as z_t)\n",
    "            eps: Finite difference step size\n",
    "            \n",
    "        Returns:\n",
    "            Jacobian matrix (shape: [n, n] where n is the total number of elements)\n",
    "        \"\"\"\n",
    "        # Flatten to work with full Jacobian\n",
    "        z_t_flat, original_shape = self._flatten_and_unflatten(z_t)\n",
    "        n = z_t_flat.shape[0]\n",
    "        \n",
    "        jacobian = torch.zeros(n, n, device=self.device, dtype=z_t.dtype)\n",
    "        \n",
    "        for i in range(n):\n",
    "            z_t_plus_flat = z_t_flat.clone()\n",
    "            z_t_plus_flat[i] += eps\n",
    "            \n",
    "            # Reshape back to original shape for denoiser evaluation\n",
    "            z_t_plus = z_t_plus_flat.reshape(original_shape)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                x_pred_plus = self.denoiser(z_t_plus.unsqueeze(0)).squeeze(0)\n",
    "            \n",
    "            # Flatten the result to compute finite difference\n",
    "            x_pred_plus_flat, _ = self._flatten_and_unflatten(x_pred_plus)\n",
    "            x_pred_flat, _ = self._flatten_and_unflatten(x_pred)\n",
    "            \n",
    "            jacobian[:, i] = (x_pred_plus_flat - x_pred_flat) / eps\n",
    "        \n",
    "        return jacobian\n",
    "    \n",
    "    def correct_vector_field_theorem1(\n",
    "        self,\n",
    "        v_hat: torch.Tensor,\n",
    "        likelihood_grad: torch.Tensor,\n",
    "        t_prime: float\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Apply Theorem 1 correction to the unconditional vector field.\n",
    "        \n",
    "        v̂_corrected = v̂(x_t) + σ_t^2 (d ln(α_t/σ_t)/dt) γ_t ∇_{x_t} ln q^app(y | x_t)\n",
    "        \n",
    "        Works with signals of arbitrary shape.\n",
    "        \n",
    "        Args:\n",
    "            v_hat: Unconditional vector field (arbitrary shape)\n",
    "            likelihood_grad: Gradient of log-likelihood (same shape as v_hat)\n",
    "            t_prime: Current time step\n",
    "            \n",
    "        Returns:\n",
    "            Corrected vector field (same shape as v_hat)\n",
    "        \"\"\"\n",
    "        sigma_t = self.probability_path.sigma(t_prime)\n",
    "        d_log_ratio_dt = self.probability_path.d_log_alpha_sigma_dt(t_prime)\n",
    "        \n",
    "        correction = (sigma_t ** 2) * d_log_ratio_dt * self.gamma_t * likelihood_grad\n",
    "        v_corrected = v_hat + correction\n",
    "        \n",
    "        return v_corrected\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        signal_shape: Tuple[int, ...],\n",
    "        t_start: float = 0.5,\n",
    "        n_steps: int = 100,\n",
    "        use_likelihood_correction: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Solve the linear inverse problem by integrating the ODE.\n",
    "        \n",
    "        Integrates from t' = t_start to t' = 1 using the corrected vector field\n",
    "        from Theorem 1.\n",
    "        \n",
    "        Dimension agnostic: Works with signals of arbitrary shape.\n",
    "        \n",
    "        Args:\n",
    "            y: Noisy measurements (shape: [m])\n",
    "            signal_shape: Shape of the signal (e.g., (n,) for 1D, (h, w) for 2D, (d, h, w) for 3D, etc.)\n",
    "            t_start: Initial time step (typically 0.5-1.0)\n",
    "            n_steps: Number of ODE integration steps\n",
    "            use_likelihood_correction: Whether to apply Theorem 1 correction\n",
    "            \n",
    "        Returns:\n",
    "            Reconstructed signal with the specified signal_shape\n",
    "        \"\"\"\n",
    "        # Initialize x_t with specified shape\n",
    "        z_t = self.initialize_xt(y, t_start, signal_shape)\n",
    "        \n",
    "        # Time steps for ODE integration from t_start to 1\n",
    "        t_steps = torch.linspace(t_start, 1.0, n_steps, device=self.device)\n",
    "        \n",
    "        # ODE integration loop\n",
    "        for i in range(len(t_steps) - 1):\n",
    "            t_prime = float(t_steps[i])\n",
    "            dt = float(t_steps[i + 1] - t_steps[i])\n",
    "            \n",
    "            # Compute r_t'^2 from probability path\n",
    "            r_t_squared = self.compute_r_t_squared(t_prime)\n",
    "            \n",
    "            # Denoiser prediction\n",
    "            with torch.no_grad():\n",
    "                x_pred = self.denoiser(z_t.unsqueeze(0)).squeeze(0)\n",
    "            \n",
    "            # Convert to unconditional vector field\n",
    "            v_hat = self.convert_to_vector_field(z_t, t_prime, x_pred)\n",
    "            \n",
    "            # Apply Theorem 1 correction\n",
    "            if use_likelihood_correction and t_prime > 1e-6:\n",
    "                likelihood_grad = self.compute_likelihood_gradient(\n",
    "                    y, x_pred, z_t, t_prime, r_t_squared\n",
    "                )\n",
    "                v_corrected = self.correct_vector_field_theorem1(\n",
    "                    v_hat, likelihood_grad, t_prime\n",
    "                )\n",
    "            else:\n",
    "                v_corrected = v_hat\n",
    "            \n",
    "            # Euler step for ODE integration\n",
    "            z_t = z_t + v_corrected * dt\n",
    "        \n",
    "        return z_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0296458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SCEGLI la PP che preferisci (non ho capito veramente la differenza)\n",
    "\n",
    "\n",
    "# Probability path implementations\n",
    "class VarianceExplodingPath(GaussianProbabilityPath):\n",
    "    \"\"\"\n",
    "    Variance-Exploding (VE) probability path from DDPM/Song et al.\n",
    "    \n",
    "    α_t = sqrt(1 / (1 + t^2))\n",
    "    σ_t = t / sqrt(1 + t^2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        def alpha_fn(t):\n",
    "            return np.sqrt(1.0 / (1.0 + t ** 2))\n",
    "        \n",
    "        def sigma_fn(t):\n",
    "            return t / np.sqrt(1.0 + t ** 2)\n",
    "        \n",
    "        super().__init__(alpha_fn, sigma_fn)\n",
    "\n",
    "\n",
    "class VariancePreservingPath(GaussianProbabilityPath):\n",
    "    \"\"\"\n",
    "    Variance-Preserving (VP) probability path.\n",
    "    \n",
    "    α_t = exp(-0.5 * ∫_0^t β(s) ds)\n",
    "    σ_t = sqrt(1 - α_t^2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta_max: float = 20.0, beta_min: float = 0.1):\n",
    "        def alpha_fn(t):\n",
    "            # Linear schedule for beta\n",
    "            beta_t = beta_min + t * (beta_max - beta_min)\n",
    "            # Cumulative integral: ∫_0^t β(s) ds ≈ beta_min*t + 0.5*(beta_max-beta_min)*t^2\n",
    "            integral = beta_min * t + 0.5 * (beta_max - beta_min) * t ** 2\n",
    "            return np.exp(-0.5 * integral)\n",
    "        \n",
    "        def sigma_fn(t):\n",
    "            alpha_t = alpha_fn(t)\n",
    "            return np.sqrt(1.0 - alpha_t ** 2)\n",
    "        \n",
    "        super().__init__(alpha_fn, sigma_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b82192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Reconstructed signal shape: torch.Size([100])\n",
      "1D Reconstructed signal (first 10 values): tensor([ 0.0334, -0.0183, -0.0201,  0.0115,  0.0025, -0.0221,  0.0907, -0.0124,\n",
      "        -0.0791, -0.0684])\n",
      "\n",
      "2D Reconstructed signal shape: torch.Size([10, 10])\n",
      "2D Reconstructed signal (first row): tensor([ 0.0143,  0.0631, -0.1100,  0.0601, -0.0096,  0.0039,  0.1075, -0.0042,\n",
      "        -0.0511,  0.0011])\n",
      "\n",
      "3D Reconstructed signal shape: torch.Size([5, 5, 4])\n",
      "3D Reconstructed signal (first slice): tensor([[ 0.0084, -0.0916, -0.0161,  0.0692],\n",
      "        [ 0.0059,  0.0239,  0.0988,  0.0091],\n",
      "        [-0.1236, -0.1145,  0.0726, -0.0056],\n",
      "        [-0.0155, -0.0116, -0.0711, -0.0462],\n",
      "        [-0.0009,  0.0235, -0.1222,  0.0327]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "# QUESTO FA GIRARE IL CODICE CON UN DENOISER DUMMY\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy denoiser for demonstration - works with any flattened dimension\n",
    "    class DummyDenoiser(nn.Module):\n",
    "        def __init__(self, signal_dim: int):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Linear(signal_dim, signal_dim)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Flatten, process, reshape back\n",
    "            shape = x.shape[1:]\n",
    "            x_flat = x.reshape(x.shape[0], -1)\n",
    "            out = self.fc(x_flat)\n",
    "            return out.reshape(x.shape)\n",
    "    \n",
    "    # Setup\n",
    "    n_signal = 100  # Signal dimension\n",
    "    m_measurement = 50  # Measurement dimension\n",
    "    \n",
    "    # Create measurement matrix A (works with flattened signal)\n",
    "    A = torch.randn(m_measurement, n_signal)\n",
    "    \n",
    "    # Create denoiser\n",
    "    denoiser = DummyDenoiser(n_signal)\n",
    "    \n",
    "    # Create probability path (VE path)\n",
    "    prob_path = VarianceExplodingPath()\n",
    "    \n",
    "    # Create solver\n",
    "    solver = ConditionalOTFlowSolver(\n",
    "        denoiser=denoiser,\n",
    "        measurement_matrix=A,\n",
    "        probability_path=prob_path,\n",
    "        sigma_y=0.01,\n",
    "        gamma_t=1.0,\n",
    "        device='cpu'\n",
    "    )\n",
    "    \n",
    "    # Create measurements y\n",
    "    y = torch.randn(m_measurement)\n",
    "    \n",
    "    # Example 1: 1D signal\n",
    "    signal_shape_1d = (n_signal,)\n",
    "    x_reconstructed_1d = solver(y, signal_shape=signal_shape_1d, t_start=0.5, n_steps=50)\n",
    "    print(f\"1D Reconstructed signal shape: {x_reconstructed_1d.shape}\")\n",
    "    print(f\"1D Reconstructed signal (first 10 values): {x_reconstructed_1d.flatten()[:10]}\")\n",
    "    \n",
    "    # Example 2: 2D signal (e.g., 10x10 image)\n",
    "    signal_shape_2d = (10, 10)\n",
    "    x_reconstructed_2d = solver(y, signal_shape=signal_shape_2d, t_start=0.5, n_steps=50)\n",
    "    print(f\"\\n2D Reconstructed signal shape: {x_reconstructed_2d.shape}\")\n",
    "    print(f\"2D Reconstructed signal (first row): {x_reconstructed_2d[0, :]}\")\n",
    "    \n",
    "    # Example 3: 3D signal (e.g., 5x5x4 volume)\n",
    "    signal_shape_3d = (5, 5, 4)\n",
    "    x_reconstructed_3d = solver(y, signal_shape=signal_shape_3d, t_start=0.5, n_steps=50)\n",
    "    print(f\"\\n3D Reconstructed signal shape: {x_reconstructed_3d.shape}\")\n",
    "    print(f\"3D Reconstructed signal (first slice): {x_reconstructed_3d[0, :, :]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
